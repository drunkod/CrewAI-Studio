# OPENAI_API_KEY="FILL-IN-YOUR-OPENAI-API-KEY"
# OPENAI_API_BASE="OPTIONAL-FILL-IN-YOUR-OPENAI-API-KEY"
# GROQ_API_KEY="FILL-IN-YOUR-GROQ_API_KEY"
# LMSTUDIO_API_BASE="http://localhost:1234/v1"
# ANTHROPIC_API_KEY="FILL-IN-YOUR-ANTHROPIC_API_KEY"
# AGENTOPS_API_KEY="FILL-IN-YOUR-AGENTOPS_API_KEY"
# OLLAMA_HOST="http://localhost:11434"
# OLLAMA_MODELS="ollama/llama3.2,ollama/llama3.1,ollama/gemma2,ollama/phi3.5"
# TODO how connect local mistral
# param endpoint: str | None = None (alias 'base_url')
# param mistral_api_key: SecretStr | None [Optional] (alias 'api_key')
# param model: str = 'mistral-small' (alias 'model_name')
# https://python.langchain.com/api_reference/mistralai/chat_models/langchain_mistralai.chat_models.ChatMistralAI.html
# MISTRAL_API_KEY=
# https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
AGENTOPS_ENABLED="False"
